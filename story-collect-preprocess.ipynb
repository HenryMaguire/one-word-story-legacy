{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Take 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Previous attempt at scraping was from this [weird site](\"http://textfiles.com/stories/\") using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FARNON \t:\t0.22% finished.\n",
      "SRE \t:\t0.22% finished.\n",
      "100west.txt \t:\t0.22% finished.\n",
      "13chil.txt \t:\t0.22% finished.\n",
      "14.lws \t:\t0.22% finished.\n",
      "16.lws \t:\t0.22% finished.\n",
      "17.lws \t:\t0.22% finished.\n",
      "18.lws \t:\t0.22% finished.\n",
      "19.lws \t:\t0.22% finished.\n",
      "20.lws \t:\t0.22% finished.\n",
      "3gables.txt \t:\t0.22% finished.\n",
      "3lpigs.txt \t:\t0.22% finished.\n",
      "3sonnets.vrs \t:\t0.22% finished.\n",
      "3student.txt \t:\t0.22% finished.\n",
      "3wishes.txt \t:\t0.22% finished.\n",
      "4moons.txt \t:\t0.22% finished.\n",
      "5orange.txt \t:\t0.22% finished.\n",
      "6ablemen.txt \t:\t0.22% finished.\n",
      "6napolen.txt \t:\t0.22% finished.\n",
      "7oldsamr.txt \t:\t0.22% finished.\n",
      "7voysinb.txt \t:\t0.22% finished.\n",
      "ab40thv.txt \t:\t0.22% finished.\n",
      "abbey.txt \t:\t0.22% finished.\n",
      "abyss.txt \t:\t0.22% finished.\n",
      "adler.txt \t:\t0.22% finished.\n",
      "adv_alad.txt \t:\t0.22% finished.\n",
      "advsayed.txt \t:\t0.22% finished.\n",
      "advtthum.txt \t:\t0.22% finished.\n",
      "aesop11.txt \t:\t0.22% finished.\n",
      "aesopa10.txt \t:\t0.22% finished.\n",
      "aircon.txt \t:\t0.22% finished.\n",
      "aisle.six \t:\t0.22% finished.\n",
      "aislesix.txt \t:\t0.22% finished.\n",
      "alad10.txt \t:\t0.22% finished.\n",
      "alissadl.txt \t:\t0.22% finished.\n",
      "altside.hum \t:\t0.22% finished.\n",
      "aluminum.hum \t:\t0.22% finished.\n",
      "aminegg.txt \t:\t0.22% finished.\n",
      "angelfur.hum \t:\t0.22% finished.\n",
      "angry_ca.txt \t:\t0.22% finished.\n",
      "antcrick.txt \t:\t0.22% finished.\n",
      "aquith.txt \t:\t0.22% finished.\n",
      "arcadia.sty \t:\t0.22% finished.\n",
      "archive \t:\t0.22% finished.\n",
      "arctic.txt \t:\t0.22% finished.\n",
      "asop \t:\t0.22% finished.\n",
      "assorted.txt \t:\t0.22% finished.\n",
      "bagel.man \t:\t0.22% finished.\n",
      "bagelman.txt \t:\t0.22% finished.\n",
      "batlslau.txt \t:\t0.22% finished.\n",
      "beast.asc \t:\t0.22% finished.\n",
      "beautbst.txt \t:\t0.22% finished.\n",
      "beggars.txt \t:\t0.22% finished.\n",
      "bern \t:\t0.22% finished.\n",
      "berternie.txt \t:\t0.22% finished.\n",
      "bestwish \t:\t0.22% finished.\n",
      "beyond.hum \t:\t0.22% finished.\n",
      "bgb.txt \t:\t0.22% finished.\n",
      "bgcspoof.txt \t:\t0.22% finished.\n",
      "bigred.hum \t:\t0.22% finished.\n",
      "bishop00.txt \t:\t0.22% finished.\n",
      "blabnove.hum \t:\t0.22% finished.\n",
      "blabnove.txt \t:\t0.22% finished.\n",
      "blackp.txt \t:\t0.22% finished.\n",
      "blackrdr \t:\t0.22% finished.\n",
      "blak \t:\t0.22% finished.\n",
      "blasters.fic \t:\t0.22% finished.\n",
      "blh.txt \t:\t0.22% finished.\n",
      "blind.txt \t:\t0.22% finished.\n",
      "blossom.pom \t:\t0.22% finished.\n",
      "blue \t:\t0.22% finished.\n",
      "bluebrd.txt \t:\t0.22% finished.\n",
      "bookem.1 \t:\t0.22% finished.\n",
      "bookem2 \t:\t0.22% finished.\n",
      "bookem3 \t:\t0.22% finished.\n",
      "brain.damage \t:\t0.22% finished.\n",
      "bram \t:\t0.22% finished.\n",
      "bran \t:\t0.22% finished.\n",
      "breaks1.asc \t:\t0.22% finished.\n",
      "breaks2.asc \t:\t0.22% finished.\n",
      "breaks3.asc \t:\t0.22% finished.\n",
      "bruce-p.txt \t:\t0.22% finished.\n",
      "buggy.txt \t:\t0.22% finished.\n",
      "buldetal.txt \t:\t0.22% finished.\n",
      "buldream.txt \t:\t0.22% finished.\n",
      "bulfelis.txt \t:\t0.22% finished.\n",
      "bulhuntr.txt \t:\t0.22% finished.\n",
      "bulironb.txt \t:\t0.22% finished.\n",
      "bullove.txt \t:\t0.22% finished.\n",
      "bulmrx.txt \t:\t0.22% finished.\n",
      "bulnland.txt \t:\t0.22% finished.\n",
      "bulnoopt.txt \t:\t0.22% finished.\n",
      "bulolli1.txt \t:\t0.22% finished.\n",
      "bulolli2.txt \t:\t0.22% finished.\n",
      "bulphrek.txt \t:\t0.22% finished.\n",
      "bulprint.txt \t:\t0.22% finished.\n",
      "bulwer.lytton \t:\t0.22% finished.\n",
      "bulzork1.txt \t:\t0.22% finished.\n",
      "bumm \t:\t0.22% finished.\n",
      "bureau.txt \t:\t0.22% finished.\n",
      "burintrv.66 \t:\t0.22% finished.\n",
      "burintrv.78 \t:\t0.22% finished.\n",
      "burintrv.92 \t:\t0.22% finished.\n",
      "burltrs \t:\t0.22% finished.\n",
      "burn \t:\t0.22% finished.\n",
      "cabin.txt \t:\t0.22% finished.\n",
      "cameloto.hum \t:\t0.22% finished.\n",
      "campfire.txt \t:\t0.22% finished.\n",
      "candle.hum \t:\t0.22% finished.\n",
      "cardcnt.txt \t:\t0.22% finished.\n",
      "ccm.txt \t:\t0.22% finished.\n",
      "charlie.txt \t:\t0.22% finished.\n",
      "chik \t:\t0.22% finished.\n",
      "clevdonk.txt \t:\t0.22% finished.\n",
      "clon \t:\t0.22% finished.\n",
      "cmoutmou.txt \t:\t0.22% finished.\n",
      "comp \t:\t0.22% finished.\n",
      "confilct.fun \t:\t0.22% finished.\n",
      "consumdr.hum \t:\t0.22% finished.\n",
      "contrad1.hum \t:\t0.22% finished.\n",
      "cooldark.sto \t:\t0.22% finished.\n",
      "cooldark.txt \t:\t0.22% finished.\n",
      "corcor.hum \t:\t0.22% finished.\n",
      "cow.exploder \t:\t0.22% finished.\n",
      "crabhern.txt \t:\t0.22% finished.\n",
      "crazy.hum \t:\t0.22% finished.\n",
      "cum \t:\t0.22% finished.\n",
      "curious.george \t:\t0.22% finished.\n",
      "cybersla.txt \t:\t0.22% finished.\n",
      "dakota.txt \t:\t0.22% finished.\n",
      "dan \t:\t0.22% finished.\n",
      "darkness.txt \t:\t0.22% finished.\n",
      "day.in.mcdonald \t:\t0.22% finished.\n",
      "deal \t:\t0.22% finished.\n",
      "deathmrs.d \t:\t0.22% finished.\n",
      "deer.txt \t:\t0.22% finished.\n",
      "descent.poe \t:\t0.22% finished.\n",
      "diaryflf.txt \t:\t0.22% finished.\n",
      "dicegame.txt \t:\t0.22% finished.\n",
      "dicksong.txt \t:\t0.22% finished.\n",
      "disco.be.fun \t:\t0.22% finished.\n",
      "discocanbefun.txt \t:\t0.22% finished.\n",
      "domain.poe \t:\t0.22% finished.\n",
      "dopedenn.txt \t:\t0.22% finished.\n",
      "dskool.txt \t:\t0.22% finished.\n",
      "dtruck.txt \t:\t0.22% finished.\n",
      "dwar \t:\t0.22% finished.\n",
      "elite.app \t:\t0.22% finished.\n",
      "elveshoe.txt \t:\t0.22% finished.\n",
      "emperor3.txt \t:\t0.22% finished.\n",
      "empnclot.txt \t:\t0.22% finished.\n",
      "empsjowk.txt \t:\t0.22% finished.\n",
      "empty.txt \t:\t0.22% finished.\n",
      "enc \t:\t0.22% finished.\n",
      "encamp01.txt \t:\t0.22% finished.\n",
      "enchdup.hum \t:\t0.22% finished.\n",
      "enginer.txt \t:\t0.22% finished.\n",
      "enya_trn.txt \t:\t0.22% finished.\n",
      "excerpt.hum \t:\t0.22% finished.\n",
      "excerpt.txt \t:\t0.22% finished.\n",
      "eyeargon.hum \t:\t0.22% finished.\n",
      "ezoff \t:\t0.22% finished.\n",
      "fable.txt \t:\t0.22% finished.\n",
      "fantas.hum \t:\t0.22% finished.\n",
      "fantasy.hum \t:\t0.22% finished.\n",
      "fantasy.txt \t:\t0.22% finished.\n",
      "fea1 \t:\t0.22% finished.\n",
      "fea2 \t:\t0.22% finished.\n",
      "fea3 \t:\t0.22% finished.\n",
      "fear.hum \t:\t0.22% finished.\n",
      "fearmnky \t:\t0.22% finished.\n",
      "fgoose.txt \t:\t0.22% finished.\n",
      "fic1 \t:\t0.22% finished.\n",
      "fic2 \t:\t0.22% finished.\n",
      "fic3 \t:\t0.22% finished.\n",
      "fic4 \t:\t0.22% finished.\n",
      "fic5 \t:\t0.22% finished.\n",
      "fic7 \t:\t0.22% finished.\n",
      "fish.txt \t:\t0.22% finished.\n",
      "fleas.txt \t:\t0.22% finished.\n",
      "flktrp.txt \t:\t0.22% finished.\n",
      "floc \t:\t0.22% finished.\n",
      "floobs.txt \t:\t0.22% finished.\n",
      "flute.txt \t:\t0.22% finished.\n",
      "flytrunk.txt \t:\t0.22% finished.\n",
      "forgotte \t:\t0.22% finished.\n",
      "fourth.fic \t:\t0.22% finished.\n",
      "fowl.death \t:\t0.22% finished.\n",
      "foxncrow.txt \t:\t0.22% finished.\n",
      "foxngrap.txt \t:\t0.22% finished.\n",
      "foxnstrk.txt \t:\t0.22% finished.\n",
      "fran \t:\t0.22% finished.\n",
      "fred.txt \t:\t0.22% finished.\n",
      "freeman.fil \t:\t0.22% finished.\n",
      "friend.s \t:\t0.22% finished.\n",
      "friends.txt \t:\t0.22% finished.\n",
      "frogp.txt \t:\t0.22% finished.\n",
      "frum \t:\t0.22% finished.\n",
      "game.txt \t:\t0.22% finished.\n",
      "gatherng.txt \t:\t0.22% finished.\n",
      "gay \t:\t0.22% finished.\n",
      "gemdra.txt \t:\t0.22% finished.\n",
      "ghost \t:\t0.22% finished.\n",
      "girl \t:\t0.22% finished.\n",
      "girlclub.txt \t:\t0.22% finished.\n",
      "glimpse1.txt \t:\t0.22% finished.\n",
      "gloves.txt \t:\t0.22% finished.\n",
      "gold3ber.txt \t:\t0.22% finished.\n",
      "goldbug.poe \t:\t0.22% finished.\n",
      "goldenp.txt \t:\t0.22% finished.\n",
      "goldfish.txt \t:\t0.22% finished.\n",
      "goldgoos.txt \t:\t0.22% finished.\n",
      "grav \t:\t0.22% finished.\n",
      "graymare.txt \t:\t0.22% finished.\n",
      "greatlrn.leg \t:\t0.22% finished.\n",
      "greedog.txt \t:\t0.22% finished.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "#url = raw_input(\"Enter a website to extract the URL's from: \")\n",
    "url = \"http://textfiles.com/stories/\"\n",
    "r  = requests.get(url)\n",
    "\n",
    "data = r.text\n",
    "\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "links = []\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "par = ''\n",
    "i = 1\n",
    "\n",
    "for l in links[1::]:\n",
    "    link = url+l\n",
    "    try:\n",
    "        s = requests.get(url+l).text\n",
    "    except:\n",
    "        pass\n",
    "    for j in s.split():\n",
    "        par+=j.strip()+u' '\n",
    "    par+=u'<END_OF_STORY>'\n",
    "    print l, \"\\t:\\t{:0.2f}% finished.\".format(100*(float(i)/len(links)))\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I have all the stories, with `<END_OF_STORY>` tags at the end of each file. These can then be split up into new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gulliver.txt \t:\t0.42% finished.\n",
      "hansgrtl.txt \t:\t0.84% finished.\n",
      "hareleph.txt \t:\t1.26% finished.\n",
      "hareporc.txt \t:\t1.68% finished.\n",
      "haretort.txt \t:\t2.10% finished.\n",
      "healer.txt \t:\t2.52% finished.\n",
      "hell4.txt \t:\t2.94% finished.\n",
      "hellmach.txt \t:\t3.36% finished.\n",
      "helmfuse.txt \t:\t3.78% finished.\n",
      "hils \t:\t4.20% finished.\n",
      "history5.txt \t:\t4.62% finished.\n",
      "hitch2.txt \t:\t5.04% finished.\n",
      "hitch3.txt \t:\t5.46% finished.\n",
      "hole2nar.txt \t:\t5.88% finished.\n",
      "holmesbk.txt \t:\t6.30% finished.\n",
      "home.fil \t:\t6.72% finished.\n",
      "hop-frog.poe \t:\t7.14% finished.\n",
      "horsdonk.txt \t:\t7.56% finished.\n",
      "horswolf.txt \t:\t7.98% finished.\n",
      "hotline1.txt \t:\t8.40% finished.\n",
      "hotline3.txt \t:\t8.82% finished.\n",
      "hotline4.txt \t:\t9.24% finished.\n",
      "hound-b.txt \t:\t9.66% finished.\n",
      "how.ernie.bert \t:\t10.08% finished.\n",
      "idi.hum \t:\t10.50% finished.\n",
      "igiv \t:\t10.92% finished.\n",
      "imagin.hum \t:\t11.34% finished.\n",
      "immortal \t:\t11.76% finished.\n",
      "immorti.hum \t:\t12.18% finished.\n",
      "imonly17.txt \t:\t12.61% finished.\n",
      "inter \t:\t13.03% finished.\n",
      "island.poe \t:\t13.45% finished.\n",
      "jackbstl.txt \t:\t13.87% finished.\n",
      "jackmac.fic \t:\t14.29% finished.\n",
      "jaynejob.asc \t:\t14.71% finished.\n",
      "jerichms.hum \t:\t15.13% finished.\n",
      "jim.asc \t:\t15.55% finished.\n",
      "keeping.insanit \t:\t15.97% finished.\n",
      "keepmodu.txt \t:\t16.39% finished.\n",
      "kharian.txt \t:\t16.81% finished.\n",
      "kneeslapper \t:\t17.23% finished.\n",
      "kneeslapper.txt \t:\t17.65% finished.\n",
      "knuckle.txt \t:\t18.07% finished.\n",
      "korea.s \t:\t18.49% finished.\n",
      "kzap.txt \t:\t18.91% finished.\n",
      "ladylust.hum \t:\t19.33% finished.\n",
      "lament.txt \t:\t19.75% finished.\n",
      "lgoldbrd.txt \t:\t20.17% finished.\n",
      "life.txt \t:\t20.59% finished.\n",
      "lil \t:\t21.01% finished.\n",
      "lionbird \t:\t21.43% finished.\n",
      "lionmane.txt \t:\t21.85% finished.\n",
      "lionmosq.txt \t:\t22.27% finished.\n",
      "lionwar.txt \t:\t22.69% finished.\n",
      "lmermaid.txt \t:\t23.11% finished.\n",
      "lmtchgrl.txt \t:\t23.53% finished.\n",
      "long1-3.txt \t:\t23.95% finished.\n",
      "lpeargrl.txt \t:\t24.37% finished.\n",
      "lrrhood.txt \t:\t24.79% finished.\n",
      "ltp \t:\t25.21% finished.\n",
      "luf \t:\t25.63% finished.\n",
      "lure.txt \t:\t26.05% finished.\n",
      "mario.txt \t:\t26.47% finished.\n",
      "mattress.txt \t:\t26.89% finished.\n",
      "mazarin.txt \t:\t27.31% finished.\n",
      "mcdonaldl.txt \t:\t27.73% finished.\n",
      "melissa.txt \t:\t28.15% finished.\n",
      "mike.txt \t:\t28.57% finished.\n",
      "mindprob.txt \t:\t28.99% finished.\n",
      "mindwar \t:\t29.41% finished.\n",
      "missing.txt \t:\t29.83% finished.\n",
      "modemhippy.txt \t:\t30.25% finished.\n",
      "monkking.txt \t:\t30.67% finished.\n",
      "monksol.txt \t:\t31.09% finished.\n",
      "mouslion.txt \t:\t31.51% finished.\n",
      "mtinder.txt \t:\t31.93% finished.\n",
      "musgrave.txt \t:\t32.35% finished.\n",
      "musibrem.txt \t:\t32.77% finished.\n",
      "mydream.txt \t:\t33.19% finished.\n",
      "myeyes \t:\t33.61% finished.\n",
      "narciss.txt \t:\t34.03% finished.\n",
      "nigel.1 \t:\t34.45% finished.\n",
      "nigel.10 \t:\t34.87% finished.\n",
      "nigel.2 \t:\t35.29% finished.\n",
      "nigel.3 \t:\t35.71% finished.\n",
      "nigel.4 \t:\t36.13% finished.\n",
      "nigel.5 \t:\t36.55% finished.\n",
      "nigel.6 \t:\t36.97% finished.\n",
      "nigel.7 \t:\t37.39% finished.\n",
      "nihgel_8.9 \t:\t37.82% finished.\n",
      "nitepeek.sto \t:\t38.24% finished.\n",
      "non2 \t:\t38.66% finished.\n",
      "non3 \t:\t39.08% finished.\n",
      "non4 \t:\t39.50% finished.\n",
      "obstgoat.txt \t:\t39.92% finished.\n",
      "omarsheh.txt \t:\t40.34% finished.\n",
      "outcast.dos \t:\t40.76% finished.\n",
      "oxfrog.txt \t:\t41.18% finished.\n",
      "paink-ws.txt \t:\t41.60% finished.\n",
      "panama.txt \t:\t42.02% finished.\n",
      "parotsha.txt \t:\t42.44% finished.\n",
      "partya.txt \t:\t42.86% finished.\n",
      "paul_har.sto \t:\t43.28% finished.\n",
      "peace.fun \t:\t43.70% finished.\n",
      "pepdegener.txt \t:\t44.12% finished.\n",
      "pepsi.degenerat \t:\t44.54% finished.\n",
      "perf \t:\t44.96% finished.\n",
      "pinocch.txt \t:\t45.38% finished.\n",
      "piracy.sto \t:\t45.80% finished.\n",
      "plescopm.txt \t:\t46.22% finished.\n",
      "poem-1.txt \t:\t46.64% finished.\n",
      "poem-2.txt \t:\t47.06% finished.\n",
      "poem-4.txt \t:\t47.48% finished.\n",
      "poplstrm.txt \t:\t47.90% finished.\n",
      "pphamlin.txt \t:\t48.32% finished.\n",
      "pregn.txt \t:\t48.74% finished.\n",
      "prince.art \t:\t49.16% finished.\n",
      "progx \t:\t49.58% finished.\n",
      "psf.txt \t:\t50.00% finished.\n",
      "psi \t:\t50.42% finished.\n",
      "psyc \t:\t50.84% finished.\n",
      "pussboot.txt \t:\t51.26% finished.\n",
      "qcarroll \t:\t51.68% finished.\n",
      "quarter.c1 \t:\t52.10% finished.\n",
      "quarter.c10 \t:\t52.52% finished.\n",
      "quarter.c11 \t:\t52.94% finished.\n",
      "quarter.c12 \t:\t53.36% finished.\n",
      "quarter.c13 \t:\t53.78% finished.\n",
      "quarter.c14 \t:\t54.20% finished.\n",
      "quarter.c15 \t:\t54.62% finished.\n",
      "quarter.c16 \t:\t55.04% finished.\n",
      "quarter.c17 \t:\t55.46% finished.\n",
      "quarter.c18 \t:\t55.88% finished.\n",
      "quarter.c19 \t:\t56.30% finished.\n",
      "quarter.c2 \t:\t56.72% finished.\n",
      "quarter.c3 \t:\t57.14% finished.\n",
      "quarter.c4 \t:\t57.56% finished.\n",
      "quarter.c5 \t:\t57.98% finished.\n",
      "quarter.c6 \t:\t58.40% finished.\n",
      "quarter.c7 \t:\t58.82% finished.\n",
      "quarter.c8 \t:\t59.24% finished.\n",
      "quarter.c9 \t:\t59.66% finished.\n",
      "quest \t:\t60.08% finished.\n",
      "quickfix \t:\t60.50% finished.\n",
      "quot \t:\t60.92% finished.\n",
      "radar_ra.txt \t:\t61.34% finished.\n",
      "rainda.txt \t:\t61.76% finished.\n",
      "reality.txt \t:\t62.18% finished.\n",
      "reap \t:\t62.61% finished.\n",
      "redragon.txt \t:\t63.03% finished.\n",
      "retrib.txt \t:\t63.45% finished.\n",
      "rid.txt \t:\t63.87% finished.\n",
      "robotech \t:\t64.29% finished.\n",
      "rock \t:\t64.71% finished.\n",
      "rocket.sf \t:\t65.13% finished.\n",
      "roger1.txt \t:\t65.55% finished.\n",
      "running.txt \t:\t65.97% finished.\n",
      "s&m_plot \t:\t66.39% finished.\n",
      "s&m_that \t:\t66.81% finished.\n",
      "safe \t:\t67.23% finished.\n",
      "sanpedr2.txt \t:\t67.65% finished.\n",
      "shoscomb.txt \t:\t68.07% finished.\n",
      "shrdfarm.txt \t:\t68.49% finished.\n",
      "shulk.txt \t:\t68.91% finished.\n",
      "sick-kid.txt \t:\t69.33% finished.\n",
      "sight.txt \t:\t69.75% finished.\n",
      "silverb.txt \t:\t70.17% finished.\n",
      "sis \t:\t70.59% finished.\n",
      "sleprncs.txt \t:\t71.01% finished.\n",
      "snow.txt \t:\t71.43% finished.\n",
      "snowmaid.txt \t:\t71.85% finished.\n",
      "snowqn1.txt \t:\t72.27% finished.\n",
      "social.vikings \t:\t72.69% finished.\n",
      "socialvikings.txt \t:\t73.11% finished.\n",
      "solitary.txt \t:\t73.53% finished.\n",
      "space.txt \t:\t73.95% finished.\n",
      "spam.key \t:\t74.37% finished.\n",
      "spectacl.poe \t:\t74.79% finished.\n",
      "spider.txt \t:\t75.21% finished.\n",
      "spiders.txt \t:\t75.63% finished.\n",
      "sqzply.txt \t:\t76.05% finished.\n",
      "sre-dark.txt \t:\t76.47% finished.\n",
      "stainles.ana \t:\t76.89% finished.\n",
      "stairdre.txt \t:\t77.31% finished.\n",
      "startrek.txt \t:\t77.73% finished.\n",
      "stsgreek \t:\t78.15% finished.\n",
      "sucker.txt \t:\t78.57% finished.\n",
      "sunday.txt \t:\t78.99% finished.\n",
      "superg1 \t:\t79.41% finished.\n",
      "szechuan \t:\t79.83% finished.\n",
      "t_zone.jok \t:\t80.25% finished.\n",
      "tailbear.txt \t:\t80.67% finished.\n",
      "tao3.dos \t:\t81.09% finished.\n",
      "taxnovel.txt \t:\t81.51% finished.\n",
      "tcoa.txt \t:\t81.93% finished.\n",
      "tctac.txt \t:\t82.35% finished.\n",
      "tearglas.txt \t:\t82.77% finished.\n",
      "telefone.txt \t:\t83.19% finished.\n",
      "terrorbears.txt \t:\t83.61% finished.\n",
      "testpilo.hum \t:\t84.03% finished.\n",
      "textfile.primer \t:\t84.45% finished.\n",
      "thanksg \t:\t84.87% finished.\n",
      "the-tree.txt \t:\t85.29% finished.\n",
      "thewave \t:\t85.71% finished.\n",
      "timem.hac \t:\t86.13% finished.\n",
      "times.fic \t:\t86.55% finished.\n",
      "timetrav.txt \t:\t86.97% finished.\n",
      "tin \t:\t87.39% finished.\n",
      "tinsoldr.txt \t:\t87.82% finished.\n",
      "toilet.s \t:\t88.24% finished.\n",
      "traitor.txt \t:\t88.66% finished.\n",
      "tree.txt \t:\t89.08% finished.\n",
      "tuc_mees \t:\t89.50% finished.\n",
      "uglyduck.txt \t:\t89.92% finished.\n",
      "unluckwr.txt \t:\t90.34% finished.\n",
      "vaincrow.txt \t:\t90.76% finished.\n",
      "vainsong.txt \t:\t91.18% finished.\n",
      "valen \t:\t91.60% finished.\n",
      "vampword.txt \t:\t92.02% finished.\n",
      "vday.hum \t:\t92.44% finished.\n",
      "veiledl.txt \t:\t92.86% finished.\n",
      "vgilante.txt \t:\t93.28% finished.\n",
      "wall.art \t:\t93.70% finished.\n",
      "wanderer.fun \t:\t94.12% finished.\n",
      "weaver.txt \t:\t94.54% finished.\n",
      "weeprncs.txt \t:\t94.96% finished.\n",
      "whgdsreg.reg \t:\t95.38% finished.\n",
      "wisteria.txt \t:\t95.80% finished.\n",
      "withdraw.cyb \t:\t96.22% finished.\n",
      "wlgirl.txt \t:\t96.64% finished.\n",
      "wolf7kid.txt \t:\t97.06% finished.\n",
      "wolfcran.txt \t:\t97.48% finished.\n",
      "wolflamb.txt \t:\t97.90% finished.\n",
      "wombat.und \t:\t98.32% finished.\n",
      "write \t:\t98.74% finished.\n",
      "wrt \t:\t99.16% finished.\n",
      "yukon.txt \t:\t99.58% finished.\n",
      "zombies.txt \t:\t100.00% finished.\n"
     ]
    }
   ],
   "source": [
    "for l in links[217::]:\n",
    "    link = url+l\n",
    "    try:\n",
    "        s = requests.get(url+l).text\n",
    "    except:\n",
    "        pass\n",
    "    for j in s.split():\n",
    "        par+=j.strip()+u' '\n",
    "    par+=u'<END_OF_STORY>'\n",
    "    print l, \"\\t:\\t{:0.2f}% finished.\".format(100*(float(i)/len(links[217::])))\n",
    "    i+=1\n",
    "\n",
    "from utils import save_obj\n",
    "save_obj(par, 'strange_texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the strange texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from utils import load_obj\n",
    "texts = load_obj('strange_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[245, 780, 3555, 1745, 935, 2883, 1968, 5344, 3335, 2534, 7774, 1226, 408, 8170, 981, 3045, 8912, 1457, 10074, 620, 9007, 2620, 11035, 1219, 2546, 2179, 2295, 961, 47422, 14431, 376, 896, 895, 6270, 470, 5632, 2454, 428, 2969, 1157, 470, 10893, 13366, 34077, 5591, 460, 2544, 1103, 1102, 19578, 8939, 1666, 21923, 529, 714, 679, 1981, 872, 3756, 2266, 16555, 9230, 9212, 9873, 788, 1945, 341, 3199, 1608, 259, 4458, 1644, 1882, 2311, 3428, 2901, 778, 1511, 31727, 29708, 22060, 13135, 737, 1211, 2622, 2500, 3988, 2716, 870, 10110, 1601, 1438, 776, 634, 8403, 5935, 1057, 5129, 2825, 29635, 3102, 2553, 5512, 368, 415, 429, 370, 576, 4057, 8892, 493, 4072, 2337, 427, 5170, 806, 2315, 529, 5014, 292, 30458, 30458, 8262, 1780, 508, 13920, 1346, 908, 92377, 29338, 3235, 45640, 1244, 2273, 1098, 779, 7765, 9853, 2017, 1028, 729, 728, 6752, 2096, 17526, 1507, 973, 22638, 561, 1759, 1279, 1612, 10333, 2691, 345, 18308, 10016, 4274, 1200, 1291, 12802, 3009, 2185, 1791, 3517, 3499, 865, 1378, 582, 1601, 224, 15638, 9553, 6222, 4290, 3021, 5914, 2787, 574, 526, 1496, 3883, 934, 493, 415, 16652, 738, 741, 505, 520, 497, 2815, 2472, 2127, 1398, 16198, 865, 768, 2232, 2069, 1927, 930, 3241, 3174, 11110, 608, 326, 1225, 16350, 10932, 1115, 1073, 4921, 635, 3862, 192, 117424, 1526, 404, 918, 495, 119692, 1526, 404, 918, 495, 3375, 1163, 22528, 421, 3029, 27547, 67457, 65320, 707, 6493, 1496, 4263, 524, 489, 722, 649, 634, 70159, 715, 3970, 4065, 2315, 2364, 4035, 681, 3579, 2250, 2036, 2864, 1232, 1359, 375, 2185, 18086, 1622, 190, 199, 4877, 3131, 2504, 781, 1496, 390, 189, 2773, 1214, 8693, 421, 380, 2718, 670, 15940, 940, 1283, 4218, 512, 4951, 963, 2643, 7405, 1243, 1221, 125, 2228, 938, 9866, 282, 594, 3246, 468, 1116, 8948, 774, 634, 241, 275, 925, 3211, 1851, 1909, 1714, 1391, 1978, 1639, 2997, 3003, 640, 499, 1707, 171, 1805, 43841, 430, 4876, 1259, 1637, 2171, 672, 114, 870, 871, 7515, 4237, 4438, 1181, 367, 346, 237, 952, 973, 2034, 650, 1000, 1325, 7280, 1793, 1471, 288, 1487, 1592, 282, 838, 2784, 1188, 289, 131, 1567, 1277, 929, 1563, 3447, 60, 247, 38, 283, 2534, 313, 26005, 239, 321, 62416, 3078, 943, 3405, 599, 3007, 173, 48514, 2186, 42996, 9156, 9800, 1502, 1155, 1750, 1129, 7852, 1022, 3380, 66194, 2021, 11623, 1898, 1311, 2391, 412, 1017, 1531, 1530, 9515, 886, 1523, 10861, 685, 1624, 12537, 6951, 647, 1026, 1707, 2024, 737, 466, 10159, 881, 1755, 595, 10820, 9825, 2987, 355, 6707, 658, 944, 3593, 1093, 245, 127, 2143, 37037, 3082, 1762, 525, 1425, 314, 175, 742, 1283, 1066, 957, 565, 1433, 2926, 170, 2482, 5461, 132809, 401, 328, 849, 1123, 1265, 13766, 3005, 1750, 1325, 531, 406, 487, 304, 3267, 16106, 12317, 0]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def remove_things(text, list_of_things):\n",
    "    text = text\n",
    "    for l in list_of_things:\n",
    "        text=text.replace(l, '')\n",
    "    return text\n",
    "list_of_junk = [u'--', '>', '<']\n",
    "stories = remove_things(texts, list_of_junk).split(u\"END_OF_STORY\")\n",
    "#stories = stories.replace()\n",
    "lens = []\n",
    "stories_t = []\n",
    "for l in stories[0::]:\n",
    "    st = nltk.word_tokenize(l)\n",
    "    lens.append(len(st))\n",
    "    stories_t.append(st)\n",
    "print lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I've scraped every text file - random stories written by people of the internet and it amounts to around 12MB of text data. This may not be enough to train a proper model but will do for development purposes. In the future I will want to write a scaper to carefully and slowly collect all the stories from https://www.gutenberg.org/wiki/Children%27s_Literature_(Bookshelf). \n",
    "\n",
    "This will involve having to use some logic to only get links which have the name attribute \n",
    "\n",
    "`\"name=ebook:<number>\"` \n",
    "\n",
    "and have \n",
    "\n",
    "`</a> (English)</li>`\n",
    "\n",
    "at the end of the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "[u'The', u'Vigilante', u'Hello', u',', u'Greetings', u',', u'Salutations', u',', u'etc', u',', u'Well', u',', u'its', u'finally', u'here', u'.', u'The', u'latest', u'chapter', u'of', u'The', u'Vigilante', u'.', u'Please', u'forgive', u'me', u'for', u'the', u'horrendous', u'delay', u'but', u'I', u\"'ve\", u'been', u'trying', u'.', u'Following', u'are', u'as', u'far', u'as', u'I', u'know', u'the', u'latest', u'versions', u'of', u'all', u'previous', u'chapters', u'of', u'my', u'epic', u'master', u'piece', u'.', u'Along', u'with', u'the', u'newest', u'.', u'I', u\"'ve\", u'tried', u'to', u'atleast', u'spell', u'check', u'the', u'thing', u',', u'so', u'at', u'least', u'that', u'much', u'is', u'right', u'.', u'As', u'for', u'continuity', u',', u'flow', u',', u'readability', u',', u'wellll', u',', u'I', u'do', u\"n't\", u'think', u'its', u'too', u'bad', u',', u'but', u'I', u\"'ve\", u'been', u'looking', u'at', u'it', u'tooo', u'much', u'lately', u'.', u'I', u'am', u'*VERY*', u'interested', u'in', u'any', u'and', u'all', u'comments', u'.', u'Please', u'let', u'me', u'know', u'what', u'you', u'like', u',', u'do', u\"n't\", u'like', u',', u'have', u'trouble', u'following', u'etc', u'.', u'I', u'am', u'receptive', u'to', u'all', u'feed', u'back', u'.', u'also', u'knowing', u'people', u'are', u'reading', u'my', u'stuff', u'keeps', u'me', u'writing', u'.', u'If', u'there', u'are', u'any', u'major', u'flow', u'errors', u'please', u'let', u'me', u'know', u',', u'even', u'send', u'the', u'offending', u'portion', u',', u'I', u'had', u'to', u'recover', u'a', u'large', u'portion', u'from', u'a', u'major', u'disk', u'failure', u'and', u'may', u'have', u'lost', u'some', u'portions', u'.', u'For', u'those', u'not', u'familure', u'with', u'the', u'Vigilante', u',', u'it', u'is', u'a', u'work', u'in', u'process', u',', u'(', u'has', u'been', u'for', u'over', u'a', u'year', u'now', u')', u'It', u'is', u'a', u'Star', u'Trek', u'the', u'Next', u'Generation', u'Universe', u'story', u'but', u'follows', u'a', u'different', u'ship', u'and', u'crew', u'.', u'The', u'Big', u'E', u'and', u'gang', u'only', u'make', u'cameo', u'appearences', u'.', u'I', u'hope', u'you', u'enjoy', u'it', u'and', u'I', u'hope', u'to', u'work', u'on', u'and', u'maybe', u'even', u'finish', u'the', u'last', u'chapter', u',', u'VI', u'``', u'Full', u'Circle', u\"''\", u'some', u'time', u'this', u'year', u'.', u'Larry', u'1160', u'Boyer', u'Rd', u'Erie', u',', u'Pa', u'16511', u'len101', u'@', u'psuvm.psu.edu', u'The', u'following', u'is', u'a', u'ST', u':', u'TNG', u'universe', u'story', u',', u'all', u'rights', u'reserved', u'.', u'Permission', u'is', u'granted']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print np.argmax(lens)\n",
    "print stories_t[442][0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These stories are noisy and crap. I'm going to get http://www.loyalbooks.com/download/text/Childhoods-Favorites-and-Fairy-Stories-by-Various.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_illustrations(text, test = False):\n",
    "    # Tailored for specific syntax for dealing with illustrations\n",
    "    \n",
    "    i = 0\n",
    "    ill_beg = 0\n",
    "    ill_fin = 0\n",
    "    picture_names = []\n",
    "    found = False\n",
    "    while i<len(text):\n",
    "        try:\n",
    "            ill_beg = text.lower().index('[illustr')\n",
    "            ill_fin = len(text[0:ill_beg])+text[ill_beg::].lower().index(']')\n",
    "            i = ill_fin\n",
    "            found = True\n",
    "        except:\n",
    "            try:\n",
    "                ill_beg = text.lower().index('[pictu')\n",
    "                ill_fin = len(text[0:ill_beg])+text[ill_beg::].lower().index(']')\n",
    "                i = ill_fin\n",
    "                found = True\n",
    "            except:\n",
    "                i = len(text)+10\n",
    "                #print \"NO illustration found\"\n",
    "        picture_names.append(text[ill_beg:ill_fin+1])\n",
    "        text=text[0:ill_beg]+text[ill_fin+1::]\n",
    "    if found:\n",
    "        pass\n",
    "        #print picture_names\n",
    "    return text\n",
    "       \n",
    "test = 'Hello we are about to see an illustration [illustration: test caption] and a picture [Picture: something].'\n",
    "s = remove_illustrations(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'American-Fairy-Tales.txt', 'Childhoods-Favorites-and-Fairy-Stories-by-Various.txt', 'Fairy-Tales-by-the-Brothers-Grimm.txt', 'Fifty-Famous-Stories-Retold-version-2-by-James-Baldwin.txt', 'not_used', 'The-Happy-Prince.txt', 'The-Junior-Classics-vol-1-by-William-Patten.txt']\n",
      "There is 2.595 MB of raw story data\n",
      "Book: American-Fairy-Tales.txt\n",
      "Book: Childhoods-Favorites-and-Fairy-Stories-by-Various.txt\n",
      "Book: Fairy-Tales-by-the-Brothers-Grimm.txt\n",
      "Book: Fifty-Famous-Stories-Retold-version-2-by-James-Baldwin.txt\n",
      "Book: The-Happy-Prince.txt\n",
      "Book: The-Junior-Classics-vol-1-by-William-Patten.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "books = os.listdir(\"book_data_edited\")\n",
    "print books\n",
    "sizes = [os.path.getsize('book_data_edited/'+i) for i in books]\n",
    "print \"There is {:0.3f} MB of raw story data\".format(sum(sizes)/1E6)\n",
    "text_data = u''\n",
    "for i in range(len(books)):\n",
    "#with open('book_data_edited/'+books[0]) as f:\n",
    "    if '.txt' in books[i]:\n",
    "        with open('book_data_edited/'+books[i]) as f:\n",
    "            print \"Book:\",books[i]\n",
    "            text_data+=remove_illustrations(unicode(f.read(), 'utf-8'))+u'<END_OF_BOOK>'\n",
    "#print nltk.word_tokenize(text_data.split(u'<END_OF_BOOK>')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We would like to also remove a fair amount more punctuation such as semi-colons, asterisks, '--', parentheses, colons, square parentheses etc. We would like to keep exclamation marks, question marks, commas and full stops. In processing the data we will also remove all capitalisation. This is a strong preprocessing step but it will reduce the vocabulary by a lot, which will make our model far easier to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A NEW_STORY_TIME Once upon a time there was a person I hated the end. NEW_STORY Hello again StoryTime\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "test = 'A NEW_STORY_TIME Once upon a time there was a person I hated the end. NEW_STORY Hello again StoryTime'\n",
    "\n",
    "print test\n",
    "def chapter_name_removal(text, rep = 'END_OF_STORY'):\n",
    "    # v1.0 Just removes fully capitalised words\n",
    "    # apart from \"I\"\n",
    "    text = re.sub(r' \\b[I]+\\b ', ' Iiii ', text) # Add some redundancy to I\n",
    "    text = re.sub(r'\\b[A-Z]+\\b', rep, text)\n",
    "    text = re.sub(r' \\b[Iiii]+\\b ', ' I ', text) # Get I back again\n",
    "    return text\n",
    "\n",
    "\n",
    "#test = u'A NEW STORY TIME Once upon a time there was a person I hated - The End. NEW STORY Hello again DoublecapsTest'\n",
    "##print chapter_name_removal(test, rep='!!')\n",
    "#text_prep = chapter_name_removal(text_prep, rep='END_OF_STORY')\n",
    "#for tex in text_prep.split(u'<END_OF_BOOK>'):\n",
    "#    \n",
    "#    print type(tex)\n",
    "#text_prep2 = chapter_name_removal(text_prep, rep = 'END_OF_STORY')\n",
    "#print nltk.word_tokenize(text_prep2.split('END_OF_BOOK')[1])[474:2000]\n",
    "#print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK 1\n",
      "BOOK 2\n",
      "BOOK 3\n",
      "BOOK 4\n",
      "BOOK 5\n",
      "BOOK 6\n",
      "BOOK 7\n",
      "475\n",
      "[u'HE BOX OF ROBBERS', u'THE GLASS DOG', u'THE QUEEN OF QUOK', u'THE GIRL WHO OWNED A BEAR', u'THE ENCHANTED TYPES', u'THE LAUGHING HIPPOPOTAMUS', u'THE MAGIC BON BONS', u'THE CAPTURE OF FATHER TIME', u'THE WONDERFUL PUMP', u'THE DUMMY THAT LIVED', u'THE KING OF THE POLAR BEARS', u'THE MANDARIN AND THE BUTTERFLY', u'SLEEP, MY TREASURE', u'E. NESBIT', u'DO YOU KNOW HOW MANY STARS?', u'NURSERY TALES', u'THE THREE BEARS', u'CINDERELLA', u'THE THREE BROTHERS', u'THE WREN AND THE BEAR', u'CHICKEN-LICKEN', u'THE FOX AND THE CAT', u'THE RATS AND THEIR SON-IN-LAW', u'THE MOUSE AND THE SAUSAGE', u'JOHNNY AND THE GOLDEN GOOSE', u'TITTY MOUSE AND TATTY MOUSE', u'TEENY TINY', u'THE SPIDER AND THE FLEA', u'THE LITTLE SHEPHERD BOY', u'THE THREE SPINNERS', u'THE CAT AND THE MOUSE IN PARTNERSHIP', u'THE SWEET SOUP', u'THE STRAW THE COAL AND THE BEAN', u'WHY THE BEAR HAS A STUMPY TAIL', u'THE THREE LITTLE PIGS', u\"CHILDREN'S FAVORITE POEMS\", u'THE THREE CHILDREN', u'ANONYMOUS', u'THE OWL AND THE PUSSY-CAT', u'II', u'PRETTY COW', u'JANE TAYLOR', u'THE TABLE AND THE CHAIR', u'II', u'III', u'IV', u'EDWARD LEAR', u\"CHILDREN'S FAVORITE STORIES\", u'HANSEL AND GRETEL', u'THE FAIR CATHERINE AND PIF-PAF POLTRIE', u'THE WOLF AND THE FOX', u'DISCREET HANS', u'PUSS IN BOOTS', u'THE ELVES AND THE SHOEMAKER', u'HANS IN LUCK', u'MASTER OF ALL MASTERS', u'BELLING THE CAT', u'LITTLE RED RIDING-HOOD', u'THE NAIL', u'JACK AND THE BEANSTALK', u'HOW TO TELL A TRUE PRINCESS', u'THE SLEEPING BEAUTY', u'OLD-FASHIONED POEMS', u'THE MAN IN THE MOON', u'LIMERICKS', u'MORE LIMERICKS', u'RUDYARD KIPLING', u'ANONYMOUS', u'THE DEAD DOLL', u'LITTLE THINGS', u'Ascribed to JULIA A. F. CARNEY', u'THE GOLDEN RULE', u'UNKNOWN', u'DO THE BEST YOU CAN', u'UNKNOWN', u'THE VOICE OF SPRING', u'THE LARK AND THE ROOK', u'THANKSGIVING DAY', u\"THE MAGPIE'S NEST\", u'A FABLE', u'THE FAIRIES OF THE CALDON LOW', u'A MIDSUMMER LEGEND', u'THE LAND OF STORY-BOOKS', u'A VISIT FROM ST. NICHOLAS', u'LITTLE ORPHANT ANNIE', u'THE CHATTERBOX', u'THE VOICE OF SPRING', u'THE HISTORY LESSON', u'SONG OF LIFE', u'THE GOOD TIME COMING', u'WINDY NIGHTS', u'THE WONDERFUL WORLD', u'HARK! HARK! THE LARK', u'WILLIAM SHAKESPEARE', u'JOG ON, JOG ON', u'WILLIAM SHAKESPEARE', u'SWEET STORY OF OLD', u'MY SHADOW', u\"BY COOL SILOAM'S SHADY RILL\", u'THE WIND IN A FROLIC', u'THE GRAVES OF A HOUSEHOLD', u'WE ARE SEVEN', u'THE BETTER LAND', u'THE JUVENILE ORATOR', u'THE FOX AND THE CROW', u'A FABLE', u'MORAL', u'LITTLE B. TAYLOR?', u'THE USE OF FLOWERS', u'CONTENTED JOHN', u\"THE OLD MAN'S COMFORTS, AND\", u'HOW HE GAINED THEM', u'THE FROST', u'THE BATTLE OF BLENHEIM', u'THE CHAMELEON', u'A FABLE', u'FROM M. DE LAMOTTE', u'JAMES MERRICK', u'THE BLACKBERRY GIRL', u'MABEL ON MIDSUMMER DAY', u'A STORY OF THE OLDEN TIME', u'PART I', u'LLEWELLYN AND HIS DOG', u\"THE SNOWBIRD'S SONG\", u\"FOR A' THAT AND A' THAT\", u'FABLES', u'FABLES FROM \\xc6SOP', u'THE GOOSE THAT LAID GOLDEN EGGS', u'THE BOYS AND THE FROGS', u'THE FOX AND THE GRAPES', u'THE TORTOISE AND THE HARE', u'THE VAIN JACKDAW', u'THE CROW AND THE PITCHER', u'THE MAN, HIS SON, AND HIS ASS', u'FABLES OF INDIA', u'ADAPTED BY P. V. RAMASWAMI RAJU', u'THE CAMEL AND THE PIG', u'THE MAN AND HIS PIECE OF CLOTH', u'THE SEA, THE FOX, AND THE WOLF', u'THE BIRDS AND THE LIME', u'THE RAVEN AND THE CATTLE', u'TINSEL AND LIGHTNING', u'THE ASS AND THE WATCH-DOG', u'THE LARK AND ITS YOUNG ONES', u'THE TWO GEMS', u'FAIRY TALES AND LAUGHTER STORIES', u'SCANDINAVIAN STORIES', u'THE HARDY TIN SOLDIER', u'BY HANS CHRISTIAN ANDERSEN', u'THE FIR TREE', u'BY HANS CHRISTIAN ANDERSEN', u'THE DARNING-NEEDLE', u'BY HANS CHRISTIAN ANDERSEN', u'THUMBELINA', u'BY HANS CHRISTIAN ANDERSEN', u'THE TINDER-BOX', u'BY HANS CHRISTIAN ANDERSEN', u'BOOTS AND HIS BROTHERS', u'BY GEORGE WEBBE DASENT', u'THE HUSBAND WHO WAS TO MIND THE HOUSE', u'BY GEORGE WEBBE DASENT', u'BUTTERCUP', u'BY GEORGE WEBBE DASENT', u'GERMAN STORIES', u'SEVEN AT ONE BLOW', u'BY WILHELM AND JAKOB GRIMM', u'ONE EYE, TWO EYES, THREE EYES', u'BY WILHELM AND JAKOB GRIMM', u'THE MUSICIANS OF BREMEN', u'BY WILHELM AND JAKOB GRIMM', u'THE FISHERMAN AND HIS WIFE', u'BY WILHELM AND JAKOB GRIMM', u'LITTLE SNOW-WHITE', u'BY WILHELM AND JAKOB GRIMM', u'THE GOOSE-GIRL', u'BY WILHELM AND JAKOB GRIMM', u'THE GOLDEN BIRD', u'BY WILHELM AND JAKOB GRIMM', u'THE WHITE CAT', u\"BY THE COMTESSE D'AULNOY\", u'THE STORY OF PRETTY GOLDILOCKS', u'TOADS AND DIAMONDS', u'ENGLISH STORIES', u'THE HISTORY OF TOM THUMB', u'ADAPTED BY ERNEST RHYS', u'JACK THE GIANT-KILLER', u'ADAPTED BY JOSEPH JACOBS', u'THE THREE SILLIES', u'ADAPTED BY JOSEPH JACOBS', u'CELTIC STORIES', u\"KING O'TOOLE AND HIS GOOSE\", u'ADAPTED BY JOSEPH JACOBS', u'THE HAUGHTY PRINCESS', u'ADAPTED BY PATRICK KENNEDY', u'JACK AND HIS MASTER', u'ADAPTED BY JOSEPH JACOBS', u\"HUDDEN AND DUDDEN AND DONALD O'NEARY\", u'ADAPTED BY JOSEPH JACOBS', u'CONNLA OF THE GOLDEN HAIR AND THE FAIRY MAIDEN', u'ADAPTED BY PATRICK WESTON JOYCE', u'ITALIAN STORIES', u\"PINOCCHIO'S ADVENTURES IN WONDERLAND1\", u'BY CARLO LORENZINI', u'II', u'GEPPETTO PLANS A WONDERFUL PUPPET', u'III', u'THE PUPPET IS NAMED PINOCCHIO', u'IV', u'THE FIRE-EATER FRIGHTENS PINOCCHIO', u'V.', u'FIRE-EATER SNEEZES AND PARDONS PINOCCHIO', u'VI', u'THE SHOWMAN BECOMES GENEROUS', u'VII', u'THE INN OF THE RED-CRAWFISH', u'VIII', u'THE PUPPET FALLS AMONG ASSASSINS', u'IX', u'THE FOX AND THE CAT', u'PINOCCHIO IS ROBBED', u'JAPANESE STORIES', u'THE STORY OF THE MAN WHO DID NOT WISH TO DIE', u'ADAPTED BY YEI THEODORA OZAKI', u'THE ACCOMPLISHED AND LUCKY TEAKETTLE', u'ADAPTED BY A. B. MITFORD', u'THE TONGUE-CUT SPARROW', u'BATTLE OF THE MONKEY AND THE CRAB', u'MOMOTARO, OR LITTLE PEACHLING', u'URASCHIMA TARO AND THE TURTLE', u'EAST INDIAN STORIES', u'THE SON OF SEVEN QUEENS', u'ADAPTED BY JOSEPH JACOBS', u\"WHO KILLED THE OTTER'S BABIES\", u'ADAPTED BY WALTER SKEAT', u'THE ALLIGATOR AND THE JACKAL', u'ADAPTED BY M. FRERE', u'THE FARMER AND THE MONEY-LENDER', u'TIT FOR TAT', u'ADAPTED BY M. FRERE', u'SINGH RAJAH AND THE CUNNING LITTLE JACKALS', u'ADAPTED BY M. FRERE', u'AMERICAN INDIAN STORIES', u'THE WHITE STONE CANOE', u'ADAPTED BY H. R. SCHOOLCRAFT', u'THE MAIDEN WHO LOVED A FISH', u'THE STAR WIFE', u'ARABIAN STORIES', u'THE STORY OF CALIPH STORK', u'PERSEVERE AND PROSPER', u'ADAPTED BY A. R. MONTALBA', u'CHINESE STORIES', u'THE MOST FRUGAL OF MEN', u'THE MOON-CAKE', u'THE LADLE THAT FELL FROM THE MOON', u'THE YOUNG HEAD OF THE FAMILY', u'A DREADFUL BOAR', u'RUSSIAN STORIES', u'KING KOJATA', u'THE STORY OF KING FROST', u'TALES FOR TINY TOTS', u'IN SEARCH OF A BABY', u'BY F. TAPSELL', u'DOLLY DIMPLE', u'BY F. TAPSELL', u'THE TALE OF PETER RABBIT', u'BY BEATRIX POTTER', u'THE MILLER, HIS SON, AND THEIR ASS', u'THE VISIT TO SANTA CLAUS LAND', u'THE GREEDY BROWNIE', u'THE BROWNIES', u'BY JULIANA HORATIA EWING', u'II', u'III', u'THE STORY OF PETER PAN', u'THE BIRTHDAY HONORS OF THE FAIRY QUEEN1', u'BY HAPGOOD MOORE', u'THE ADVENTURES OF CHANTICLEER AND PARTLET', u'THE WEDDING OF MRS FOX', u'ING ALFRED AND THE CAKES.', u'KING ALFRED AND THE BEGGAR.', u'KING CANUTE ON THE SEASHORE.', u'THE SONS OF WILLIAM THE CONQUEROR.', u'THE WHITE SHIP.', u'KING JOHN AND THE ABBOT.', u'I. THE THREE QUESTIONS.', u'A STORY OF ROBIN HOOD.', u'BRUCE AND THE SPIDER.', u'THE BLACK DOUGLAS.', u'THREE MEN OF GOTHAM.', u'OTHER WISE MEN OF GOTHAM.', u'THE MILLER OF THE DEE.', u'SIR PHILIP SIDNEY.', u'THE UNGRATEFUL SOLDIER.', u'SIR HUMPHREY GILBERT.', u'SIR WALTER RALEIGH.', u'POCAHONTAS.', u'GEORGE WASHINGTON AND HIS HATCHET.', u'GRACE DARLING.', u'THE STORY OF WILLIAM TELL.', u'ARNOLD WINKELRIED.', u'THE BELL OF ATRI.', u'HOW NAPOLEON CROSSED THE ALPS.', u'THE STORY OF CINCINNATUS.', u'THE STORY OF REGULUS.', u\"CORNELIA'S JEWELS.\", u'ANDROCLUS AND THE LION.', u'HORATIUS AT THE BRIDGE.', u'JULIUS C\\xc6SAR.', u'THE SWORD OF DAMOCLES.', u'DAMON AND PYTHIAS.', u'A LACONIC ANSWER.', u'THE UNGRATEFUL GUEST.', u'ALEXANDER AND BUCEPHALUS.', u'DIOGENES THE WISE MAN.', u'THE BRAVE THREE HUNDRED.', u'SOCRATES AND HIS HOUSE.', u'THE KING AND HIS HAWK.', u'DOCTOR GOLDSMITH.', u'THE KINGDOMS.', u'THE BARMECIDE FEAST.', u'THE ENDLESS TALE.', u'THE BLIND MEN AND THE ELEPHANT.', u'MAXIMILIAN AND THE GOOSE BOY.', u'THE INCHCAPE ROCK.', u'WHITTINGTON AND HIS CAT.', u'I. THE CITY.', u'CASABIANCA.', u'ANTONIO CANOVA.', u'PICCIOLA.', u'MIGNON.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "list_of_punct = ['<', '>', '(', ')', '[', ']', ':', ';', '--','*']\n",
    "\n",
    "def remove_things(text, list_of_things):\n",
    "    text = text\n",
    "    for l in list_of_things:\n",
    "        text=text.replace(l, '')\n",
    "    return text\n",
    "\n",
    "text_prep = remove_things(text_data, list_of_punct)\n",
    "\n",
    "i = 1\n",
    "tokenized_stories = []\n",
    "skipped = []\n",
    "delims = []\n",
    "chapter_count = 0\n",
    "for book in text_prep.split('END_OF_BOOK'):\n",
    "    # First find where paragraphs end\n",
    "    print \"BOOK\", i\n",
    "    delim = ''\n",
    "    p_delim = ''\n",
    "    if len(book.split('\\r\\n\\r\\n\\r\\n')) == 1:\n",
    "        delim = '\\n\\n\\n'\n",
    "        p_delim = '\\n'\n",
    "        #print len(book.split(delim))\n",
    "    elif len(book.split('\\n\\n\\n')) ==1:\n",
    "        delim = '\\r\\n\\r\\n\\r\\n'\n",
    "        p_delim = '\\r\\n'\n",
    "        #print len(book.split(delim))\n",
    "    \n",
    "    chapters = book.split(delim)\n",
    "    tok_story = []\n",
    "    for chapter in chapters:\n",
    "        # Short Chapters tend to actually be Chapter titles (not needed)\n",
    "        # Titles are always FULLY CAPITALISED\n",
    "        if len(chapter)<=1:\n",
    "            pass\n",
    "        elif len(chapter)<500:\n",
    "            questionable = chapter.split(p_delim)\n",
    "            chapter_clean = ''\n",
    "            for par in questionable:\n",
    "                if len(par) >1:\n",
    "                    capitalised_count = 0 # count up whether word was capital\n",
    "                    for char in par.strip('\\n'):\n",
    "                        if char.lower()!=char:\n",
    "                            capitalised_count+=1\n",
    "                    # If most of chars are capitals\n",
    "                    if float(capitalised_count)/len(par) >0.4:\n",
    "                        skipped.append(par.strip('\\n'))\n",
    "                    else:\n",
    "                        chapter_clean +=par.strip('\\n')\n",
    "            if len(word_tokenize(chapter_clean))>1:\n",
    "                tok_story.append([u'<SOS>']+word_tokenize(chapter_clean.lower())+[u'<EOS>'])\n",
    "                chapter_count+=1\n",
    "        else:\n",
    "            if len(word_tokenize(chapter))>1:\n",
    "                tok_story.append([u'<SOS>']+word_tokenize(chapter.lower())+[u'<EOS>'])\n",
    "                chapter_count+=1\n",
    "    tokenized_stories+=tok_story\n",
    "    #b = ' END_PAR '.join()\n",
    "    #text_prep2.append(b)\n",
    "    delims.append(delim)\n",
    "    #try:\n",
    "    #    print book.index(\"Project Gutenberg\")\n",
    "    #except:\n",
    "    #    pass\n",
    "    #print len(b)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "'''print '\\n***********\\n'.join([book1.split(delims[n])[0], \n",
    "                           book1.split(delims[n])[1],\n",
    "                           book1.split(delims[n])[2],\n",
    "                           book1.split(delims[n])[3],\n",
    "                           book1.split(delims[n])[4]])\n",
    "'''\n",
    "#print len(skipped)\n",
    "#lens = [len(i) for i in skipped]\n",
    "#print skipped[np.argmax([len(i) for i in skipped])]\n",
    "#print sorted(zip(lens, skipped), reverse=True)[0:10]\n",
    "#print len(text_prep2)\n",
    "#print bookn.split(delims[n])[0]\n",
    "\n",
    "print chapter_count\n",
    "print skipped\n",
    "\n",
    "#print len(book1)\n",
    "#nltk.word_tokenize()#[474:2000][1270:1290]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15195 words in the full vocabulary.\n",
      "There are 6104 hapax legomenon.\n",
      "Vocab size of 8000 restricts to 52.65 percent of total vocab.\n",
      "Percentage of whole corpus covered by vocab: 98.53\n",
      "***  SEQUENCE STATS ***\n",
      "Min: 4  Max: 79643  Mean: 1188.71   Median: 497.0   Mode: 36   1st Quart.: 180.0   IQR: 1157.0   3rd Quart.: 1337.0\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from utils import sequence_stats\n",
    "import itertools\n",
    "text =[' '.join(s) for s in tokenized_stories]\n",
    "#print text[0:1000]\n",
    "\n",
    "freq = FreqDist(itertools.chain(*tokenized_stories))\n",
    "\n",
    "def covered_with_vocab(freq_dist, vocab_size):\n",
    "    total_words = len(freq_dist.keys())\n",
    "    print \"There are {} words in the full vocabulary.\".format(total_words)\n",
    "    total_length = sum(freq_dist.values())\n",
    "    print \"There are {} hapax legomenon.\".format(len(freq_dist.hapaxes()))\n",
    "    \n",
    "    print \"Vocab size of {} restricts to {:0.2f} percent of total vocab.\".format(\n",
    "                                                vocab_size, 100*(float(min(total_words, vocab_size))/total_words))\n",
    "    vocab = dict(freq_dist.most_common(vocab_size))\n",
    "    #print \"vocab size= \",len(vocab.keys())\n",
    "    covered_with_vocab = sum(vocab.values()) # Total of frequencies\n",
    "    print \"Percentage of whole corpus covered by vocab: {:0.2f}\".format(\n",
    "                                                    100*(covered_with_vocab/float(total_length)))\n",
    "\n",
    "covered_with_vocab(freq, 8000)\n",
    "sequence_stats(tokenized_stories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8004\n"
     ]
    }
   ],
   "source": [
    "EOS_ID = 3 # End of chapter/story token\n",
    "SOS_ID = 2 # Start of chapter/story\n",
    "UNK_ID = 1 # Unknown word (out of vocabulary)\n",
    "PAD_ID = 0 # \n",
    "static_IDs = [PAD_ID, UNK_ID, SOS_ID, EOS_ID]\n",
    "def word_to_ids(freq_dist, vocab_size):\n",
    "    #vocab = dict(freq_dist.most_common(vocab_size))\n",
    "    s = sorted(freq_dist.most_common(vocab_size+2), key=lambda x: x[1], reverse=True)\n",
    "    # Remove word from vocab if it occurs fewer than 2 time\n",
    "    vocab = [w for w, i in s if ((w not in (u'<SOS>', u'<EOS>')) and (i >1))]\n",
    "    \n",
    "    #vocab = zip(*)\n",
    "    vocab_size = len(vocab)\n",
    "    print vocab_size\n",
    "    # IDs begin at 4 because <EOS>=1, <UNK>=2 and <EOS>=3\n",
    "    word_to_ids = dict([(word, i+max(static_IDs)+1) for i, word in enumerate(vocab)])\n",
    "    word_to_ids[u'<SOS>'] = SOS_ID\n",
    "    word_to_ids[u'<UNK>'] = UNK_ID\n",
    "    word_to_ids[u'<EOS>'] = EOS_ID\n",
    "    word_to_ids[u'<PAD>'] = PAD_ID\n",
    "    id_to_words = dict((idx, word) for word, idx in word_to_ids.items())\n",
    "    return word_to_ids, id_to_words, vocab_size\n",
    "\n",
    "word2id, id2word, vocab_size = word_to_ids(freq, 8000)\n",
    "print len(word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 1 2\n"
     ]
    }
   ],
   "source": [
    "print word2id[u'<PAD>'], word2id[u'<EOS>'], word2id[u'<UNK>'], word2id[u'<SOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED REMOVING UNKNOWN WORDS\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "#import numba\n",
    "from functools import partial\n",
    "#from numba import jit\n",
    "\n",
    "def replace_id_func(sequence, **kwargs):\n",
    "    # Parallelisable version of word to ID replacer\n",
    "    word2id=kwargs['wtid']\n",
    "    ids_sent = [] # sentence with words replaced by ids\n",
    "    for token in sequence:\n",
    "        ids_sent.append(word2id[token])\n",
    "    return ids_sent\n",
    "#@jit\n",
    "def replace_with_word_id(text, wtid):\n",
    "    '''\n",
    "    take the list of lists, find FreqDist, replace any\n",
    "    out of vocabulary words with <UNK> whilst giving each\n",
    "    token a numerical ID, return new list of lists'''\n",
    "    ti = time.time()\n",
    "    prep_text = []\n",
    "    pool = Pool(processes = 3)\n",
    "    kwargs = {'wtid': wtid}\n",
    "    # Parallelise process, set buffer size to restrict memory usage\n",
    "    id_text = [i for i in pool.imap(partial(replace_id_func, **kwargs), text, chunksize=20000)]\n",
    "    print \"Replacing words with IDs took {} seconds.\".format(time.time()-ti)\n",
    "    return id_text\n",
    "\n",
    "#text_ids = replace_with_word_id(text_tok, word2id)\n",
    "#print len(word2id.keys())\n",
    "#print tokenized_stories[0:100]\n",
    "stories_tokenized = tokenized_stories\n",
    "\n",
    "for i, story_i in enumerate(tokenized_stories):\n",
    "    stories_tokenized[i] = [w if w in word2id else '<UNK>' for w in story_i]\n",
    "    \"\"\"\n",
    "    #for word in story:\n",
    "    #    if word not in word2id:\n",
    "    #        print word, word2id[word]\n",
    "    tok_story = []\n",
    "    for w in story_i:\n",
    "        if w in word2id.keys():\n",
    "            tok_story.append(w)\n",
    "        else:\n",
    "            tok_story.append('<EOS>')\n",
    "    stories_tokenized[i] = tok_story\n",
    "    \"\"\"\n",
    "print \"FINISHED REMOVING UNKNOWN WORDS\"\n",
    "\n",
    "#for i, story_i in enumerate(tokenized_stories):\n",
    "#    stories_tokenized[i] = [word2id[w] for w in story_i]\n",
    "#    #stories_tokenized[i] = [w if w in word2id else '<UNK>' for w in story_i]\n",
    "#\n",
    "#print stories_tokenized[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475\n"
     ]
    }
   ],
   "source": [
    "print len(stories_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED SWAPPING OUT IDs\n",
      "[2, 64, 44, 3021, 8, 317, 1587, 411, 19, 1734, 4, 25, 16, 304, 19, 978, 14, 190, 78, 4, 26, 44, 1141, 111, 200, 7, 2175, 1, 14, 1, 5, 1, 7653, 1847, 447, 59, 5, 836, 46, 1, 4721, 7, 446, 1, 46, 167, 74, 24, 190, 166, 1, 8, 138, 22, 26, 9, 116, 1129, 7, 1935, 14, 34, 5, 3042, 4, 28, 1243, 7, 16, 14, 1926, 3684, 46, 82, 48, 7, 28, 26, 7362, 4, 21, 478, 124, 45, 1142, 15, 5, 140, 6, 163, 106, 5, 41, 256, 25, 7362, 24, 9, 6222, 1826, 7, 13, 65, 18, 361, 4, 831, 4, 67, 20, 151, 1912, 5, 1, 8, 463, 9, 398, 8, 2175, 1, 46, 256, 40, 12, 21, 126, 1587, 7, 13, 1, 33, 4, 12, 212, 5, 227, 7, 13, 18, 713, 263, 1973, 5, 98, 155, 4, 330, 4, 6, 138, 5, 1275, 4, 26, 20, 100, 42, 1315, 7, 12, 13, 201, 4, 20, 224, 62, 19, 4, 11, 559, 4, 831, 4, 12, 23, 5, 904, 686, 4, 6, 164, 78, 8, 2368, 5, 1734, 30, 22, 345, 4, 1193, 1587, 166, 411, 15, 5, 234, 140, 4, 6, 1643, 15, 4, 57, 5, 863, 7, 5, 41, 256, 872, 9, 502, 3011, 15, 22, 439, 809, 4, 2912, 9, 502, 4425, 15, 22, 5433, 6, 639, 8, 13, 483, 4827, 12, 30, 22, 536, 1703, 2919, 7, 43, 21, 1040, 19, 15, 5, 2823, 14, 9, 1404, 46, 1, 19, 24, 119, 95, 584, 26, 1668, 4, 32, 21, 1060, 21, 65, 1897, 16, 6, 133, 16, 15, 552, 7, 493, 30, 54, 1120, 4, 5, 256, 688, 5, 2743, 1050, 8, 5, 234, 264, 238, 5, 858, 7, 16, 14, 97, 1246, 59, 174, 1, 1173, 6, 14, 622, 6, 937, 7, 407, 5, 1119, 52, 2980, 11, 3516, 6, 6692, 4, 6508, 11, 80, 1, 4, 472, 11, 5058, 1874, 4, 3505, 11, 1, 2231, 6, 130, 1, 6, 2674, 11, 113, 111, 1425, 2179, 7, 172, 1, 140, 148, 91, 2823, 11, 54, 1143, 4, 32, 20, 709, 33, 5304, 16, 7, 5, 1404, 46, 140, 24, 95, 1199, 4, 25, 106, 9, 1062, 1587, 153, 16, 78, 102, 15, 9, 611, 302, 5, 234, 1262, 7, 21, 499, 16, 48, 6, 1002, 19, 299, 16, 14, 9, 315, 1127, 917, 75, 1306, 1724, 24, 313, 102, 69, 3118, 312, 6, 312, 1, 1587, 14, 1158, 4, 15, 1052, 7, 2229, 24, 145, 22, 94, 16, 44, 82, 88, 49, 14, 64, 1275, 8, 16, 4, 291, 1306, 1724, 424, 16, 8, 908, 1, 242, 10, 360, 118, 6, 88, 54, 2408, 1306, 4, 56, 14, 9, 1051, 1097, 4, 24, 292, 57, 1, 8, 1944, 5470, 6, 24, 131, 95, 147, 69, 1191, 7, 5, 41, 256, 163, 34, 5, 917, 3853, 4, 76, 19, 16, 24, 59, 2303, 3380, 22, 1876, 7, 16, 14, 166, 1, 273, 129, 2229, 46, 2796, 1, 14, 1, 37, 102, 30, 6392, 1, 2022, 7, 16, 14, 621, 4, 152, 4, 26, 39, 1587, 404, 8, 2037, 44, 320, 11, 16, 21, 153, 21, 68, 33, 1857, 16, 9, 554, 7, 25, 49, 14, 9, 221, 15, 5, 257, 11, 5, 2121, 26, 9, 1275, 7, 21, 1769, 8, 6446, 5, 1973, 4, 6, 99, 19, 16, 65, 138, 9, 675, 234, 1275, 8, 367, 16, 7, 43, 4, 28, 18, 194, 4269, 4, 5, 41, 256, 1689, 8, 367, 1306, 1724, 46, 234, 570, 6, 86, 58, 14, 15, 16, 7, 26, 85, 71, 37, 1545, 4, 6, 41, 859, 71, 151, 28, 1545, 28, 5, 332, 11, 177, 7, 13, 20, 62, 119, 1, 1306, 1724, 224, 254, 93, 98, 4, 12, 21, 110, 7, 13, 1935, 23, 112, 19, 96, 2406, 108, 45, 448, 29, 7, 67, 20, 107, 24, 9, 1275, 12, 21, 468, 6, 2380, 22, 41, 340, 261, 2702, 28, 21, 1040, 9, 234, 758, 11, 2594, 35, 5, 2983, 15, 5, 3959, 2515, 7, 27, 52, 11, 37, 2733, 6, 7963, 531, 44, 11, 55, 65, 6414, 5, 3520, 917, 31, 21, 266, 70, 5, 1050, 4, 153, 5, 758, 6, 360, 30, 16, 8, 5, 2823, 7, 43, 21, 170, 70, 101, 5, 1, 570, 6, 132, 1109, 44, 1275, 106, 200, 15, 5, 1545, 80, 1973, 7, 96, 52, 152, 288, 4, 25, 293, 52, 152, 321, 7, 44, 65, 77, 57, 5, 1973, 25, 65, 33, 563, 200, 1190, 32, 258, 19, 21, 1860, 26, 9, 81, 19, 21, 65, 131, 144, 16, 48, 84, 7, 25, 34, 123, 4, 39, 5, 758, 14, 519, 951, 4, 91, 1, 4, 2477, 3167, 1275, 853, 821, 57, 5, 1973, 7, 30, 9, 609, 11, 427, 1587, 318, 5, 1275, 30, 327, 340, 43, 21, 147, 9, 1005, 13, 4215, 4, 12, 6, 5, 198, 382, 5, 621, 1324, 266, 53, 11, 180, 216, 3255, 31, 5, 41, 256, 3217, 102, 5, 961, 11, 5, 917, 91, 1520, 4, 6, 5, 477, 19, 314, 22, 157, 1104, 22, 8, 1990, 98, 15, 2429, 7, 1397, 6, 1164, 9, 74, 1, 117, 69, 5, 917, 4, 1467, 48, 87, 5, 646, 4, 820, 17, 2407, 6, 43, 104, 109, 17, 715, 6, 1424, 2316, 8, 5, 1140, 227, 7, 10, 14, 875, 6, 1495, 6, 17, 352, 421, 1743, 6743, 111, 1, 7, 43, 200, 74, 1, 69, 5, 917, 4, 3747, 6, 3105, 17, 157, 105, 9, 3199, 1, 7, 10, 14, 11, 530, 1504, 6, 17, 847, 421, 28, 1743, 6743, 28, 19, 11, 5, 189, 7, 161, 1587, 2485, 1, 34, 5, 1690, 477, 9, 435, 74, 2603, 69, 5, 917, 7, 10, 24, 5, 296, 1, 28, 17, 1772, 4, 25, 14, 579, 6, 564, 7, 37, 174, 52, 626, 15, 9, 1545, 1106, 7, 27, 1450, 579, 1, 11, 283, 2576, 4967, 30, 171, 4, 6, 2826, 6180, 11, 4840, 2951, 30, 377, 2219, 7, 102, 66, 2467, 52, 4096, 773, 3319, 11, 283, 6, 779, 6, 489, 4, 161, 66, 1687, 24, 1361, 1, 30, 369, 4, 1, 3005, 4, 69, 75, 2590, 3449, 11, 1, 3319, 7, 27, 24, 234, 171, 2644, 15, 66, 657, 6, 2980, 11, 3322, 6, 5393, 15, 66, 1, 7, 66, 157, 52, 315, 6, 2223, 6, 27, 1450, 116, 4, 1398, 1, 4, 4085, 34, 5, 2674, 105, 9, 384, 46, 480, 7, 13, 50, 31, 25, 18, 52, 621, 4, 12, 789, 5, 564, 44, 4, 39, 10, 24, 603, 70, 17, 2576, 2904, 6, 3988, 5, 1897, 69, 17, 4840, 6180, 7, 13, 6, 18, 3631, 51, 37, 48, 11, 1933, 7, 12, 13, 16, 14, 1, 4, 7168, 4, 12, 5245, 5, 1495, 74, 4, 2245, 13, 5, 1324, 11, 5, 917, 1656, 51, 70, 87, 18, 7, 275, 20, 1515, 18, 50, 1, 7, 12, 13, 28, 26, 51, 4, 12, 23, 5, 3854, 74, 4, 3029, 1829, 9, 6782, 6, 1, 16, 4, 13, 18, 108, 5275, 20, 45, 95, 73, 3565, 345, 26, 312, 32, 62, 33, 42, 2825, 7, 12, 13, 18, 108, 119, 1392, 15, 5, 2823, 4, 12, 23, 1587, 4, 5241, 243, 34, 477, 11, 5, 6782, 7, 13, 18, 204, 187, 5, 140, 35, 255, 7, 12, 5, 3854, 74, 4, 56, 24, 33, 1002, 22, 101, 4, 34, 54, 2167, 318, 8, 5, 256, 6, 1424, 7, 13, 561, 9, 297, 1, 16, 4, 12, 23, 10, 4, 13, 20, 100, 1, 50, 6782, 4, 12, 6, 10, 380, 16, 35, 5, 646, 6, 5277, 16, 30, 17, 547, 7, 13, 56, 71, 18, 40, 12, 126, 1587, 4, 56, 242, 76, 24, 95, 152, 1140, 8, 42, 529, 7, 13, 4247, 177, 8, 7645, 2267, 4, 12, 23, 5, 1495, 74, 4, 1, 17, 715, 3324, 7, 13, 54, 38, 3340, 4, 12, 5, 564, 74, 2384, 13, 6, 54, 38, 1975, 4, 12, 5, 3854, 74, 1424, 13, 6, 20, 115, 1599, 7, 85, 71, 174, 1, 2039, 7, 12, 13, 2039, 31, 12, 120, 1587, 4, 30, 9, 228, 11, 2439, 7, 13, 1057, 7, 531, 15, 37, 5, 218, 49, 71, 33, 174, 130, 2039, 32, 950, 6, 1398, 28, 2267, 4, 12, 23, 1599, 4, 2609, 7, 13, 798, 38, 32, 4, 12, 23, 5, 564, 74, 4, 3155, 4810, 7, 13, 25, 16, 46, 558, 31, 12, 789, 1587, 7, 13, 280, 4, 329, 4, 12, 212, 1599, 7, 13, 85, 71, 2364, 6, 1, 558, 7, 531, 15, 37, 5, 218, 18, 68, 33, 196, 174, 230, 113, 558, 129, 475, 56, 76, 714, 101, 18, 7, 12, 13, 798, 38, 32, 4, 12, 23, 5, 564, 74, 4, 1, 7, 13, 25, 18, 124, 119, 42, 32, 558, 4, 12, 23, 5, 256, 12, 1, 31, 12, 1599, 1442, 70, 17, 157, 6, 3730, 7, 13, 2157, 31, 12, 2855, 1975, 4, 30, 9, 4961, 228, 7, 13, 798, 38, 9, 391, 398, 4, 12, 23, 7168, 4, 956, 4, 6, 1235, 17, 352, 15, 17, 340, 7, 13, 20, 41, 110, 4, 12, 3225, 1599, 4, 15, 9, 287, 771, 59, 1, 4, 12, 254, 8, 42, 32, 1, 59, 9, 297, 31, 275, 4, 531, 18, 613, 1, 7, 18, 108, 2413, 4, 831, 4, 19, 251, 5415, 148, 91, 1991, 7, 26, 88, 71, 85, 8, 42, 2039, 4, 139, 51, 423, 4, 1041, 85, 71, 558, 40, 12, 1587, 14, 4519, 6, 911, 22, 137, 4, 3045, 7, 43, 21, 1040, 271, 7, 13, 18, 755, 119, 908, 2039, 184, 351, 4, 12, 23, 21, 4, 13, 291, 18, 71, 76, 15, 3534, 7, 12, 13, 3534, 31, 12, 120, 5, 174, 4, 261, 7, 13, 478, 7, 18, 71, 35, 2505, 7673, 4, 15, 6518, 7, 1306, 1724, 313, 18, 135, 69, 3118, 15, 54, 917, 7, 12, 5, 2039, 421, 1033, 3246, 59, 54, 6724, 7, 3340, 170, 70, 35, 91, 80, 767, 30, 9, 771, 1, 6, 3332, 17, 1941, 30, 9, 779, 1859, 1304, 7, 1975, 6, 1599, 181, 98, 87, 5, 917, 6, 163, 34, 22, 30, 1317, 1870, 6, 2220, 157, 7, 39, 10, 24, 2577, 2142, 117, 1599, 613, 7, 13, 73, 1306, 1724, 148, 1033, 3751, 177, 4, 12, 10, 23, 4, 1, 7, 13, 10, 148, 484, 177, 69, 251, 2330, 3118, 4, 89, 2039, 71, 5228, 7746, 4, 6, 237, 177, 8, 9, 573, 362, 89, 85, 100, 33, 142, 413, 8, 2096, 111, 88, 136, 8, 423, 26, 9, 7439, 7, 12, 13, 798, 38, 32, 31, 12, 23, 5, 564, 74, 4, 1, 17, 781, 4119, 7, 13, 6, 85, 24, 1782, 176, 240, 1, 15, 3118, 31, 12, 23, 1975, 4, 1, 7, 13, 531, 1306, 1724, 350, 8, 1, 18, 4, 12, 3603, 1587, 7, 13, 71, 49, 4, 43, 4, 64, 2039, 15, 6518, 40, 12, 126, 1599, 7, 13, 97, 4, 12, 212, 5, 256, 4, 6393, 15, 22, 563, 4, 13, 85, 62, 33, 589, 55, 2039, 7, 12, 13, 43, 58, 100, 85, 62, 26, 9, 833, 40, 12, 929, 1975, 4, 1, 7, 13, 9, 83, 954, 90, 42, 247, 15, 9, 234, 7761, 541, 4, 12, 23, 5, 227, 7, 13, 50, 121, 38, 9, 1, 12, 5, 2039, 3807, 4, 13, 6, 50, 4363, 1921, 38, 9, 3467, 3360, 7, 12, 13, 461, 4, 12, 23, 1599, 4, 13, 19, 38, 9, 92, 1, 7, 5, 3467, 709, 8, 42, 6387, 4, 1790, 15, 3118, 7, 12, 13, 2040, 31, 12, 1078, 1975, 7, 13, 43, 18, 68, 62, 130, 300, 4, 12, 883, 1587, 4, 1, 7, 13, 18, 68, 42, 1, 230, 35, 1, 7651, 4, 111, 5678, 15, 9, 6262, 1445, 7, 96, 203, 273, 511, 1, 8, 1508, 9, 833, 7, 12, 5, 2039, 911, 66, 677, 956, 7, 13, 85, 71, 33, 1983, 26, 176, 269, 4, 12, 23, 1599, 7, 13, 251, 1088, 38, 8, 2096, 7, 12, 1587, 404, 8, 208, 7, 13, 16, 38, 675, 391, 8, 144, 7877, 15, 5, 1, 3042, 4, 12, 21, 23, 4, 12, 25, 18, 204, 511, 6261, 7, 12, 13, 64, 31, 12, 120, 1975, 4, 30, 1569, 1, 13, 85, 108, 33, 1, 251, 369, 1395, 7, 2039, 85, 45, 253, 95, 4, 6, 2039, 85, 108, 908, 31, 12, 13, 798, 38, 32, 31, 12, 739, 5, 564, 74, 7, 13, 273, 15, 6518, 49, 108, 42, 203, 8, 2096, 4, 12, 1475, 1599, 4, 30, 1, 7, 1587, 14, 4434, 7, 13, 20, 208, 27, 45, 37, 95, 2616, 4, 12, 21, 7965, 7, 13, 43, 85, 90, 2096, 5, 1004, 4, 26, 85, 45, 2831, 6, 1, 1359, 5, 2325, 4, 12, 23, 1975, 7, 13, 201, 4, 265, 201, 4, 265, 31, 12, 5483, 5, 256, 13, 220, 79, 1306, 1724, 254, 897, 18, 135, 15, 54, 917, 40, 12, 5, 2039, 298, 3261, 7, 13, 19, 38, 58, 85, 124, 105, 8, 142, 4, 12, 1205, 1599, 4, 2035, 7, 13, 25, 64, 44, 47, 254, 142, 4, 26, 1306, 1724, 14, 341, 161, 1069, 5470, 15, 1, 4, 12, 21, 883, 4, 30, 1, 7, 13, 43, 85, 108, 2832, 251, 1997, 6, 2096, 8, 5, 364, 11, 251, 1, 4, 12, 23, 1599, 7, 13, 32, 116, 28, 85, 71, 1172, 8, 251, 2330, 1, 85, 709, 33, 42, 1799, 7, 12, 13, 798, 38, 32, 31, 12, 120, 5, 564, 74, 7, 13, 371, 31, 85, 47, 949, 76, 7, 139, 177, 2096, 5, 140, 85, 71, 15, 7, 12, 13, 92, 31, 12, 984, 5, 482, 6, 586, 8, 66, 344, 7, 1975, 318, 1, 87, 5, 227, 7, 13, 908, 135, 31, 12, 10, 1989, 7, 13, 67, 18, 1857, 44, 1203, 73, 513, 47, 42, 35, 73, 216, 137, 31, 12, 43, 10, 1078, 4, 15, 9, 1, 287, 13, 62, 119, 42, 431, 19, 46, 5, 127, 37, 2039, 702, 8, 66, 6645, 7, 25, 11, 559, 85, 65, 119, 983, 9, 167, 297, 238, 184, 3728, 7, 12, 13, 11, 559, 33, 4, 12, 23, 1599, 7, 5, 564, 74, 499, 9, 234, 805, 69, 17, 2046, 6, 1, 16, 94, 17, 137, 7, 13, 1, 31, 12, 10, 1, 4, 2657, 7, 13, 1, 31, 12, 120, 1975, 4, 15, 9, 950, 287, 7, 13, 4904, 8, 251, 3659, 31, 12, 1, 1599, 7, 6, 43, 5, 174, 1318, 383, 804, 2625, 6, 661, 7002, 70, 5, 7457, 30, 1, 5393, 15, 66, 340, 6, 2223, 3322, 620, 66, 888, 4, 1193, 1587, 1572, 30, 418, 6, 152, 4961, 8, 273, 609, 26, 284, 7, 88, 116, 21, 662, 411, 15, 5, 2823, 21, 131, 246, 4, 25, 1565, 21, 147, 5, 1, 3302, 11, 5, 1793, 2039, 6, 99, 55, 336, 53, 5, 1050, 15, 1076, 1, 7, 37, 1667, 621, 5845, 11, 4682, 15, 66, 683, 4, 6, 3340, 14, 5306, 9, 5882, 2183, 35, 5, 469, 11, 9, 2468, 11, 22, 122, 46, 364, 262, 2604, 7, 1599, 60, 198, 30, 91, 7245, 11, 7373, 4, 9, 3167, 1, 6, 5, 1878, 1652, 7, 1975, 24, 5, 791, 1, 4, 5, 758, 11, 1, 69, 5, 1, 4, 9, 1621, 1162, 6, 1935, 46, 1682, 1, 7, 13, 201, 4, 427, 31, 12, 23, 1599, 4, 1006, 70, 17, 2123, 13, 16, 38, 937, 8, 2096, 112, 113, 7, 12, 13, 201, 4, 1, 31, 12, 23, 1975, 25, 10, 139, 5, 1162, 782, 35, 17, 4895, 6, 640, 132, 909, 407, 15, 6799, 4, 161, 10, 4645, 1744, 403, 15, 5, 6580, 1619, 7, 13, 85, 45, 136, 1617, 4, 12, 883, 1599, 4, 1307, 5, 5882, 2183, 161, 3340, 1078, 17, 1, 8, 5, 1337, 13, 6, 37, 69, 44, 140, 31, 54, 3534, 108, 42, 9, 414, 221, 7, 12, 30, 9, 1, 10, 43, 290, 117, 9, 334, 11, 5, 2183, 6, 2354, 5, 4498, 8, 17, 2872, 7, 1523, 37, 174, 170, 87, 5, 646, 6, 5173, 5, 2183, 161, 1587, 163, 35, 956, 7, 13, 85, 124, 45, 9, 1112, 4, 12, 1475, 1975, 13, 26, 85, 108, 1445, 251, 4682, 15, 9, 737, 221, 7, 90, 18, 173, 177, 11, 9, 1122, 1112, 40, 12, 10, 126, 1587, 7, 13, 49, 46, 9, 1, 1112, 4, 12, 21, 183, 4, 13, 25, 16, 46, 15, 1, 7, 18, 65, 42, 672, 8, 818, 35, 5, 7651, 9, 116, 81, 8, 144, 49, 7, 12, 5, 174, 2039, 163, 2236, 6, 1, 66, 2183, 3924, 4, 25, 5, 198, 382, 27, 52, 4062, 59, 5, 4448, 11, 5, 1, 1, 4, 75, 14, 147, 2982, 273, 15, 5, 1, 2823, 7, 13, 58, 46, 19, 40, 12, 1883, 1599, 4, 15, 9, 2944, 287, 4, 28, 5, 174, 4227, 8, 66, 344, 30, 1288, 6215, 7, 1587, 164, 8, 5, 319, 6, 99, 16, 14, 107, 5, 1, 4, 56, 24, 850, 9, 2641, 15, 5, 570, 6, 292, 78, 84, 7, 25, 5, 5152, 156, 22, 91, 1120, 11, 88, 8, 144, 1479, 11, 22, 4638, 2039, 4, 32, 21, 132, 5587, 22, 340, 28, 67, 15, 83, 2626, 6, 120, 48, 12, 16, 46, 5, 3467, 31, 12, 5, 1004, 163, 34, 44, 200, 30, 6101, 2729, 4, 6, 3340, 126, 4, 1, 12, 71, 49, 186, 11, 55, 40, 12, 13, 9, 395, 6, 682, 31, 12, 789, 1587, 4, 106, 3862, 8, 1547, 55, 7, 13, 43, 85, 71, 341, 31, 12, 1205, 1975, 13, 26, 85, 68, 131, 1085, 32, 186, 6, 323, 7, 12, 13, 71, 27, 3110, 40, 12, 929, 1599, 4, 56, 14, 3869, 28, 67, 537, 7, 13, 201, 4, 280, 4, 12, 23, 21, 7, 13, 27, 45, 3398, 6, 3395, 6, 5393, 6, 4070, 1, 12, 12, 6, 58, 40, 12, 1883, 3340, 7, 13, 6, 1, 31, 12, 5, 174, 558, 1046, 3729, 1438, 6, 1975, 23, 4, 15, 9, 1494, 287, 12, 20, 828, 27, 47, 507, 177, 533, 6, 33, 133, 177, 8, 5, 1, 7, 20, 45, 95, 145, 244, 1, 71, 2516, 3041, 4, 56, 71, 1, 6, 950, 7, 12, 13, 798, 38, 32, 31, 12, 2855, 5, 564, 74, 4, 30, 9, 733, 7, 549, 1587, 318, 69, 5, 319, 7, 13, 18, 71, 50, 492, 4, 71, 18, 33, 40, 12, 21, 126, 7, 13, 85, 71, 2858, 31, 12, 183, 1599, 7, 13, 85, 1, 18, 31, 12, 120, 1975, 7, 13, 85, 65, 430, 26, 18, 31, 12, 1078, 3340, 4, 487, 10, 14, 94, 8, 430, 3572, 7, 13, 43, 20, 47, 740, 18, 4, 12, 23, 5, 256, 7, 13, 88, 40, 12, 126, 5, 174, 4, 30, 44, 287, 7, 13, 144, 98, 57, 5, 917, 4, 12, 21, 23, 7, 13, 20, 47, 43, 453, 5, 1324, 4, 32, 27, 47, 42, 1666, 8, 196, 18, 7, 12, 27, 163, 407, 5, 264, 15, 9, 4809, 6, 1, 127, 4, 25, 21, 789, 12, 18, 108, 42, 1338, 31, 27, 47, 125, 42, 135, 8, 7449, 18, 7, 12, 43, 3340, 586, 57, 5, 917, 6, 205, 564, 87, 5, 754, 7, 1975, 1574, 15, 198, 6, 4571, 117, 15, 5, 98, 257, 7, 1599, 601, 106, 6548, 8, 1660, 22, 210, 8, 5, 256, 15, 9, 2664, 1106, 7, 43, 1587, 164, 53, 8, 3719, 70, 5, 1324, 4, 25, 68, 33, 154, 16, 742, 7, 13, 18, 108, 2993, 70, 4, 12, 21, 23, 8, 55, 7, 3340, 3729, 7, 13, 20, 115, 615, 50, 364, 4, 831, 4, 12, 23, 1599, 4, 56, 14, 3565, 5, 469, 12, 25, 795, 85, 1983, 15, 72, 2816, 101, 4, 5, 917, 76, 1580, 675, 321, 26, 177, 7, 12, 13, 798, 38, 32, 31, 12, 60, 5, 1, 287, 11, 5, 564, 74, 69, 5, 754, 7, 13, 20, 142, 58, 1721, 53, 5, 264, 4, 12, 23, 1975, 7, 13, 58, 40, 12, 929, 1599, 4, 5626, 7, 13, 5, 2183, 4, 12, 360, 1975, 7, 13, 798, 38, 32, 31, 12, 60, 69, 5, 754, 4, 15, 3698, 7180, 7, 43, 1587, 170, 87, 5, 1324, 6, 1656, 16, 70, 30, 37, 22, 2102, 7, 8, 22, 83, 1293, 5, 1973, 387, 4, 6, 4, 2550, 70, 4, 21, 5202, 37, 22, 1031, 6, 318, 5, 1275, 7, 54, 355, 124, 1526, 177, 33, 8, 5380, 15, 2474, 19, 62, 33, 4643, 177, 7, 26, 24, 1587, 1, 69, 1814, 1306, 1, 3520, 917, 21, 65, 33, 45, 95, 672, 8, 426, 1571, 37, 5, 4682, 5, 1004, 24, 237, 57, 5, 2823, 7]\n"
     ]
    }
   ],
   "source": [
    "from utils import save_obj\n",
    "X_train = np.asarray([[word2id[w] for w in sent[:-1]] for sent in tokenized_stories])\n",
    "y_train = np.asarray([[word2id[w] for w in sent[1:]] for sent in tokenized_stories])\n",
    "print \"FINISHED SWAPPING OUT IDs\"\n",
    "data_dic = {'X_train': X_train, 'y_train': y_train, 'word2id': word2id, 'id2word': id2word}\n",
    "save_obj(data_dic, \"data_dic\")\n",
    "print X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
